{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "nG6m3Q9DBHV1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")\n"
      ],
      "metadata": {
        "id": "gcHFulM05q0y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries"
      ],
      "metadata": {
        "id": "33R54QYjCMAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-openai \\\n",
        "               faiss-cpu tiktoken python-dotenv"
      ],
      "metadata": {
        "id": "8F7iZIBmB8_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e2215c-02e8-47f5-ac57-ce946f1b6b32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m471.0/485.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-huggingface\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD9Zw0DqKGKK",
        "outputId": "359566fe-d0fb-404f-eabd-f58f41e0ab40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.2.6)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.59)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.11.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.16.0)\n",
            "Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-text-splitters\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M63tYpEtYvg",
        "outputId": "e992482a-10d0-41cb-d60f-3753e6dae9d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.6)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "hPcswe0tExci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cfb876-6fa4-4225-f769-a68e1e4b01c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n"
      ],
      "metadata": {
        "id": "eSQNrUjGy49o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1a - Indexing (Document Ingestion)"
      ],
      "metadata": {
        "id": "0ZZrs-ijCTYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "video_id = \"6S59Y0ckTm4\"\n",
        "api = YouTubeTranscriptApi()\n",
        "\n",
        "try:\n",
        "    transcript_list = api.fetch(video_id, languages=['en'])\n",
        "\n",
        "    # ✅ Timestamp-aware transcript storage\n",
        "    transcript_with_timestamps = []\n",
        "\n",
        "    for chunk in transcript_list:\n",
        "        transcript_with_timestamps.append({\n",
        "            \"text\": chunk.text,\n",
        "            \"start\": chunk.start,\n",
        "            \"end\": chunk.start + chunk.duration\n",
        "        })\n",
        "\n",
        "    # Optional: print sample\n",
        "    for item in transcript_with_timestamps[:10]:\n",
        "        print(\n",
        "            f\"[{item['start']:.2f}s → {item['end']:.2f}s] {item['text']}\"\n",
        "        )\n",
        "\n",
        "except TranscriptsDisabled:\n",
        "    print(\"No captions available for this video.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0GvwDjQvGuT",
        "outputId": "1b63b199-969b-4434-ed7f-d52914f3ef70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56s → 5.48s] hello all my name is Kish naak and\n",
            "[2.80s → 7.20s] welcome to my YouTube channel so guys in\n",
            "[5.48s → 9.36s] one of our previous video i' had already\n",
            "[7.20s → 11.48s] shown you how you can actually fine tune\n",
            "[9.36s → 14.24s] Lama 2 model with the with your own\n",
            "[11.48s → 16.44s] custom data set and uh over there we\n",
            "[14.24s → 17.60s] learned about or we saw code that were\n",
            "[16.44s → 20.04s] related to something called as\n",
            "[17.60s → 22.48s] quantization Laura CLA techniques and\n",
            "[20.04s → 25.84s] all right and all these techniques are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1b - Indexing (Text Splitting)"
      ],
      "metadata": {
        "id": "eKkcYsaOCrRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seconds_to_timestamp(seconds):\n",
        "    minutes = int(seconds // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    return f\"{minutes:02d}:{secs:02d}\"\n"
      ],
      "metadata": {
        "id": "LqmGSiIsE-Ft"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "CHUNK_CHAR_LIMIT = 800\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "documents = []\n",
        "\n",
        "current_text = \"\"\n",
        "current_start = None\n",
        "\n",
        "for item in transcript_with_timestamps:\n",
        "    # initialize start time for a new chunk\n",
        "    if current_start is None:\n",
        "        current_start = item[\"start\"]\n",
        "\n",
        "    current_text += \" \" + item[\"text\"]\n",
        "\n",
        "    # when chunk size reached → create document\n",
        "    if len(current_text) >= CHUNK_CHAR_LIMIT:\n",
        "        documents.append(\n",
        "            Document(\n",
        "                page_content=current_text.strip(),\n",
        "                metadata={\n",
        "                    \"start\": current_start,\n",
        "                    \"end\": item[\"end\"]\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # keep overlap text for next chunk\n",
        "        current_text = current_text[-CHUNK_OVERLAP:]\n",
        "        current_start = item[\"start\"]\n",
        "\n",
        "# handle remaining text\n",
        "if current_text.strip():\n",
        "    documents.append(\n",
        "        Document(\n",
        "            page_content=current_text.strip(),\n",
        "            metadata={\n",
        "                \"start\": current_start,\n",
        "                \"end\": transcript_with_timestamps[-1][\"end\"]\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"Total chunks created: {len(documents)}\")\n",
        "print(\"Sample chunk:\")\n",
        "print(documents[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVjGQSdxAjz9",
        "outputId": "550a23d0-28a9-4cba-d54e-13f29c180b41"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 45\n",
            "Sample chunk:\n",
            "page_content='hello all my name is Kish naak and welcome to my YouTube channel so guys in one of our previous video i' had already shown you how you can actually fine tune Lama 2 model with the with your own custom data set and uh over there we learned about or we saw code that were related to something called as quantization Laura CLA techniques and all right and all these techniques are super important if you also want to train or fine-tune your own llm models with your own custom data set now when I showed the code right when I executed that particular code many people had actually requested about to explain the theoretical in-depth intuition about it and that is what I'm actually going to do uh the best thing is that when I learned about this theoretical intuition and I'm doing it from past 2 to 3 months' metadata={'start': 0.56, 'end': 50.76}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dm9sfpQFnF1",
        "outputId": "8214c028-8791-4292-adcb-bbd63071e5b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[35]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYlrcBrkFO-N",
        "outputId": "4a3d2a8b-3536-43ad-84e0-b063b6e06cd0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'start': 1543.0, 'end': 1596.9599999999998}, page_content=\"see this five right this is basically called as 0 point right so there are two important parameters that we specifically talk with respect to quantization one is 0 point for for the above one since we have a symmetrical distribution here the 0 point was Zero only and the scale was 3.92 in this particular case since it asymmetrical distribution here here we have a 0 point as nothing but five but scale is 4.0 so this two parameters we usually require to perform quantization okay and these are some of the examples that I have shown you to just give you an idea like how quantization basically happens and super important in terms of understanding is the simple equations you'll be able to understand how things are basically working right at the end of the day understand quantization is a simple process of converting that high uh full\")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
      ],
      "metadata": {
        "id": "8xYFK7WXC2Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "mnMOsFGi06bZ"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "jYXeS5T7FrC4"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.index_to_docstore_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWYkp-NmFSVF",
        "outputId": "b4407a9b-8de8-43a1-84b4-5f603a2d098a"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '3866d189-0aed-4d56-bd52-6f6eb4b6fd36',\n",
              " 1: 'e85a1d9e-bb06-4e8d-b246-59fdc6f20c78',\n",
              " 2: '3dfead64-c9af-4627-855c-e0ea5c9dc926',\n",
              " 3: '4bf7a29c-30bf-481f-b088-daf0f6c7d4d5',\n",
              " 4: '3cc47b86-09ea-43b7-bb1f-ab4b31ef3053',\n",
              " 5: '65d8b3ee-c649-4d53-bd81-2e78e1cce523',\n",
              " 6: 'bedca127-45b8-4901-a19c-b95bdfc2ff13',\n",
              " 7: '0fb78066-9644-472d-843b-747b97a9d606',\n",
              " 8: '92d9a289-151d-4031-a933-cb585df5f55b',\n",
              " 9: 'fcc19fb7-5a02-4fd2-870d-47ebf54fcf53',\n",
              " 10: 'a7a8d449-6941-44c8-a270-02848f9a18ae',\n",
              " 11: '66efaef9-c8da-4451-ab0b-eda249d281a8',\n",
              " 12: '16f7b5f8-a0b1-4623-a387-71af7532f8c4',\n",
              " 13: '6ed41135-7f08-499c-bac2-ce64e823cde3',\n",
              " 14: 'd1a74670-eced-4678-9346-56f8b5fd2a0d',\n",
              " 15: 'ccf8cc51-147c-4540-a777-235c1a1ac231',\n",
              " 16: '74ab4149-3049-4209-8b33-4ed876046def',\n",
              " 17: 'cb022258-b4e9-4aee-9078-8b1a7623fa46',\n",
              " 18: '2543f868-92ae-4f56-bcbd-afc2860ce705',\n",
              " 19: '391299f4-6b15-4b0f-814b-1409e4a487b0',\n",
              " 20: 'da7d12e3-c8a9-4582-af8f-48a7d461b4d2',\n",
              " 21: 'c65e11d7-2a53-4921-a033-c6eeafc1a98f',\n",
              " 22: '99b1f9de-b128-4dbc-8643-cc14912e6cf1',\n",
              " 23: 'c057e778-e057-4892-9eac-24429c3215a8',\n",
              " 24: '931c9020-8e05-4b96-90e7-f993a2fc0d27',\n",
              " 25: '19946c38-6e2d-4802-aca0-4ec1ad097ec1',\n",
              " 26: '31237e9c-8897-4fa5-92f6-39f7e8d4a4b8',\n",
              " 27: 'a8a0fd0d-72cd-4fbf-8b13-e8a1e61a567b',\n",
              " 28: '63601a67-28eb-404c-bcd2-cf099445fbf5',\n",
              " 29: '9a6809d5-0a71-4013-a362-829a1c952631',\n",
              " 30: '0749e10c-ee3e-4a3d-9a3b-685581947149',\n",
              " 31: '3ccea4f1-f3c3-4b29-a1eb-2d8eceb3c103',\n",
              " 32: '7d82851d-a266-4120-944a-25824d8c6763',\n",
              " 33: '5ca5e8fe-2ff2-4134-8c31-1906c1350c9e',\n",
              " 34: 'd4ee0e57-6f2c-435c-9456-0b55e9443b89',\n",
              " 35: '92e182db-8430-4dd3-8a05-2f79b13979f5',\n",
              " 36: '84ea1701-4e90-4710-9945-5e9a0034a015',\n",
              " 37: 'e2bf47a0-d3f6-4d9c-87aa-539c4da28475',\n",
              " 38: '2d5eb80d-fcaa-4bf2-8feb-1c77da4729c0',\n",
              " 39: '3a2e05f5-87b3-4b3b-9a50-bc312ccfd8f1',\n",
              " 40: '2e83062e-f650-42e2-b698-c08561dbf9bc',\n",
              " 41: '00c0070d-4e5c-4cff-ac79-4b43083d9a68',\n",
              " 42: '24e8503b-ee9c-415f-8b4f-ef86f8d7a3b2',\n",
              " 43: '16b4f20f-392c-46a4-b9bd-3634d769aa00',\n",
              " 44: 'e9790e6b-19be-43c0-bbe5-17c93b693b10'}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.get_by_ids(['16b4f20f-392c-46a4-b9bd-3634d769aa00'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxokTcWEGGAo",
        "outputId": "5672b03f-f725-479d-8dd3-5f194dbf4631"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='16b4f20f-392c-46a4-b9bd-3634d769aa00', metadata={'start': 1926.88, 'end': 1965.159}, page_content=\"-tuning our data and then we create our quantized models right so this is the basic difference so all the fine-tuning technique that I will probably show you in the future will be of this type that is quantisation aware training so that we do not lose much data accuracy so I hope you got an idea with respect to all these three techniques guys going ahead there are two important techniques that we really need to understand one is claa and one is Laura so this techniques specifically will be also understanding with respect to fine tuning so I hope you got an idea just get to know about all these things guys this is important because someone if someone ask in the interview what exactly it is then you'll be able to understand it very much easily and again explaining all these things will be important if you're really interesting\")]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Retrieval"
      ],
      "metadata": {
        "id": "Zez1650EDN9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "KEuoGUYOF3oG"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcose8VuGFAv",
        "outputId": "0e516167-97be-40cf-95e0-373dc206fb6d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7aadb0557860>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('explain how many models Quantization')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvrsq08TGGNk",
        "outputId": "c03922a8-b2b1-45db-d57a-3fcce30ea423"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='2e83062e-f650-42e2-b698-c08561dbf9bc', metadata={'start': 1777.08, 'end': 1834.36}, page_content=\"hen convert this into a quantized model right the second technique that we have written over is quantization aware training okay quantization aware training so let's talk about this quantization aware training this is also called as CU so this is also called as Q8 okay so we can write it as Q8 okay quantization aware technique this is basically called as ptq ptq okay so over here what is the exact difference we'll try to see between this two okay now in quantisation a training what happens see over here what is the problem if I probably perform calibration and if I create a quantise model there is a loss of data and because of this what will happen is that the accuracy will also decrease okay for any use cases but in the case of quantisation aware training okay you'll be able to see that we will be taking our train\"),\n",
              " Document(id='3a2e05f5-87b3-4b3b-9a50-bc312ccfd8f1', metadata={'start': 1732.64, 'end': 1783.6399999999999}, page_content=\"ights data whatever weights data is basically there in this particular preer model and then we convert this into a quantized model okay so once we apply this process then only we'll be able to get the quantise model and then we can use this entire model for any use cases okay for any use cases right this is a simple mechanism with respect to post training quation see understand post trining basically means I already have a pre-train model where my weights are fixed I don't need to change those weights I will just take or download those weights I will take this weights data apply the calibration and then convert this into a quantized model right the second technique that we have written over is quantization aware training okay quantization aware training so let's talk about this quantization aware\"),\n",
              " Document(id='92e182db-8430-4dd3-8a05-2f79b13979f5', metadata={'start': 1543.0, 'end': 1596.9599999999998}, page_content=\"see this five right this is basically called as 0 point right so there are two important parameters that we specifically talk with respect to quantization one is 0 point for for the above one since we have a symmetrical distribution here the 0 point was Zero only and the scale was 3.92 in this particular case since it asymmetrical distribution here here we have a 0 point as nothing but five but scale is 4.0 so this two parameters we usually require to perform quantization okay and these are some of the examples that I have shown you to just give you an idea like how quantization basically happens and super important in terms of understanding is the simple equations you'll be able to understand how things are basically working right at the end of the day understand quantization is a simple process of converting that high uh full\"),\n",
              " Document(id='84ea1701-4e90-4710-9945-5e9a0034a015', metadata={'start': 1591.399, 'end': 1640.399}, page_content=\"erstanding is the simple equations you'll be able to understand how things are basically working right at the end of the day understand quantization is a simple process of converting that high uh full single Precision or full Precision floating Point 32 bits into small bits you know it can be uh unsigned integer 8 it can be signed integer 8 if we say signed integer eight then what will happen it is that it will be ranging betweenus 128 to 127 and based on that you can specifically apply the formula right now let's go go ahead see we had already discussed about these two topics one is this and second one we wanted to discuss about calibration now this squeezing that you could see right from here to here to here here to here we squeezing it right this squeezing process is basically called as\")]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 - Augmentation"
      ],
      "metadata": {
        "id": "F8y0wRmoDSVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n"
      ],
      "metadata": {
        "id": "x2P2AlJ0GN5L"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a video transcript analysis assistant.\n",
        "\n",
        "Answer STRICTLY using the transcript context below.\n",
        "Do NOT use outside knowledge.\n",
        "\n",
        "TASK:\n",
        "1. Decide whether the user's topic is discussed in the video.\n",
        "2. If YES:\n",
        "   - Clearly explain what is discussed\n",
        "   - Give a short summary relevant to the question\n",
        "   - List the EXACT video timestamps where it is discussed\n",
        "3. If NO:\n",
        "   - Say only: \"NO. Sorry, the topic is not discussed in this video.\"\n",
        "\n",
        "RULES:\n",
        "- Start your answer with YES or NO\n",
        "- Timestamps MUST be in MM:SS format\n",
        "- Use ONLY timestamps present in the context\n",
        "\n",
        "Transcript context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "2-NeLx9wFHzw"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question          = \"is the topic about plastic is discussed ?\"\n",
        "retrieved_docs    = retriever.invoke(question)"
      ],
      "metadata": {
        "id": "WI9BOZQwGizf"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfv8yNFsK_GN",
        "outputId": "abae79a9-06bd-46f8-d5cc-de41e66aa33b"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='3dfead64-c9af-4627-855c-e0ea5c9dc926', metadata={'start': 79.96, 'end': 124.119}, page_content='e going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of the most important interview questions will be something related to fine tuning and what is the techniques that is usually used behind right so what all things we are going to cover in this video in this video we going to talk about quantization um specifically when I say quantization it is all about model quantization because if you remember in our Lama 2 code right when we doing the fine tuning here you could see that we had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we'),\n",
              " Document(id='e85a1d9e-bb06-4e8d-b246-59fdc6f20c78', metadata={'start': 45.76, 'end': 84.36}, page_content=\"the theoretical in-depth intuition about it and that is what I'm actually going to do uh the best thing is that when I learned about this theoretical intuition and I'm doing it from past 2 to 3 months it's quite amazing guys you now this is where that machine learning era is probably coming where I used to upload a lot of theoretical in-depth geometrical intuitions regarding various machine learning algorithms similarly here here also in the series of videos in this video we are going to discuss about quantization now what exactly is quantisation we going to discuss about that in the upcoming video we are going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of\"),\n",
              " Document(id='3cc47b86-09ea-43b7-bb1f-ab4b31ef3053', metadata={'start': 154.84, 'end': 195.959}, page_content=\"alf precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at the end of the day llms are also deep learning neural networks in the form of Transformers or Bird right then we are going to discuss about what exactly is calibration uh this is also called as like calibration in model quantization right we are going to also make sure that we are going to see some problems right how we can actually do calibration then there is different different modes of quation right uh first of all I will explain you the definition then only you'll be able to understand in modes of quation we're going to discuss about two types one is post training Quant and Quant aware training right so these all\"),\n",
              " Document(id='4bf7a29c-30bf-481f-b088-daf0f6c7d4d5', metadata={'start': 119.84, 'end': 160.4}, page_content=\"e had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we are specifically doing this I will be explaining about that right so with respect to each and every parameters definitely I will explain you the theoretical intuition and later on you just go ahead and see my previous video with respect to all the coding now you everything will make sense okay so what exactly is quation we're going to discuss you know uh we're going to discuss about full Precision half precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at\")]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n\".join(\n",
        "    f\"[Timestamp: {seconds_to_timestamp(doc.metadata['start'])} \"\n",
        "    f\"to {seconds_to_timestamp(doc.metadata['end'])}]\\n\"\n",
        "    f\"{doc.page_content}\"\n",
        "    for doc in retrieved_docs\n",
        ")\n",
        "context_text"
      ],
      "metadata": {
        "id": "bKwpvAo5G_Pk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "e5f7af04-3897-48f9-f54f-0b78b48e9f12"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Timestamp: 01:19 to 02:04]\\ne going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of the most important interview questions will be something related to fine tuning and what is the techniques that is usually used behind right so what all things we are going to cover in this video in this video we going to talk about quantization um specifically when I say quantization it is all about model quantization because if you remember in our Lama 2 code right when we doing the fine tuning here you could see that we had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we\\n\\n[Timestamp: 00:45 to 01:24]\\nthe theoretical in-depth intuition about it and that is what I'm actually going to do uh the best thing is that when I learned about this theoretical intuition and I'm doing it from past 2 to 3 months it's quite amazing guys you now this is where that machine learning era is probably coming where I used to upload a lot of theoretical in-depth geometrical intuitions regarding various machine learning algorithms similarly here here also in the series of videos in this video we are going to discuss about quantization now what exactly is quantisation we going to discuss about that in the upcoming video we are going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of\\n\\n[Timestamp: 02:34 to 03:15]\\nalf precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at the end of the day llms are also deep learning neural networks in the form of Transformers or Bird right then we are going to discuss about what exactly is calibration uh this is also called as like calibration in model quantization right we are going to also make sure that we are going to see some problems right how we can actually do calibration then there is different different modes of quation right uh first of all I will explain you the definition then only you'll be able to understand in modes of quation we're going to discuss about two types one is post training Quant and Quant aware training right so these all\\n\\n[Timestamp: 01:59 to 02:40]\\ne had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we are specifically doing this I will be explaining about that right so with respect to each and every parameters definitely I will explain you the theoretical intuition and later on you just go ahead and see my previous video with respect to all the coding now you everything will make sense okay so what exactly is quation we're going to discuss you know uh we're going to discuss about full Precision half precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ],
      "metadata": {
        "id": "_bikWKZWDiqB"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOFVVAbLYvU",
        "outputId": "7b58fbe4-0bb8-4c7a-d709-c9bbdc6999f4"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='\\nYou are a video transcript analysis assistant.\\n\\nAnswer STRICTLY using the transcript context below.\\nDo NOT use outside knowledge.\\n\\nTASK:\\n1. Decide whether the user\\'s topic is discussed in the video.\\n2. If YES:\\n   - Clearly explain what is discussed\\n   - Give a short summary relevant to the question\\n   - List the EXACT video timestamps where it is discussed\\n3. If NO:\\n   - Say only: \"NO. Sorry, the topic is not discussed in this video.\"\\n\\nRULES:\\n- Start your answer with YES or NO\\n- Timestamps MUST be in MM:SS format\\n- Use ONLY timestamps present in the context\\n\\nTranscript context:\\n[Timestamp: 01:19 to 02:04]\\ne going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of the most important interview questions will be something related to fine tuning and what is the techniques that is usually used behind right so what all things we are going to cover in this video in this video we going to talk about quantization um specifically when I say quantization it is all about model quantization because if you remember in our Lama 2 code right when we doing the fine tuning here you could see that we had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we\\n\\n[Timestamp: 00:45 to 01:24]\\nthe theoretical in-depth intuition about it and that is what I\\'m actually going to do uh the best thing is that when I learned about this theoretical intuition and I\\'m doing it from past 2 to 3 months it\\'s quite amazing guys you now this is where that machine learning era is probably coming where I used to upload a lot of theoretical in-depth geometrical intuitions regarding various machine learning algorithms similarly here here also in the series of videos in this video we are going to discuss about quantization now what exactly is quantisation we going to discuss about that in the upcoming video we are going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of\\n\\n[Timestamp: 02:34 to 03:15]\\nalf precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at the end of the day llms are also deep learning neural networks in the form of Transformers or Bird right then we are going to discuss about what exactly is calibration uh this is also called as like calibration in model quantization right we are going to also make sure that we are going to see some problems right how we can actually do calibration then there is different different modes of quation right uh first of all I will explain you the definition then only you\\'ll be able to understand in modes of quation we\\'re going to discuss about two types one is post training Quant and Quant aware training right so these all\\n\\n[Timestamp: 01:59 to 02:40]\\ne had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we are specifically doing this I will be explaining about that right so with respect to each and every parameters definitely I will explain you the theoretical intuition and later on you just go ahead and see my previous video with respect to all the coding now you everything will make sense okay so what exactly is quation we\\'re going to discuss you know uh we\\'re going to discuss about full Precision half precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at\\n\\nQuestion:\\nis the topic about plastic is discussed ?\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Generation"
      ],
      "metadata": {
        "id": "MxxcV2C_DXqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.format(\n",
        "    context=context_text,\n",
        "    question=question\n",
        ")\n"
      ],
      "metadata": {
        "id": "pfbZSVbuEMPK"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chat_model.invoke(final_prompt)\n",
        "print(answer.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX6vxSoUHBok",
        "outputId": "b46db6c7-00b7-417c-9b00-0c81589a2caf"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NO. Sorry, the topic is not discussed in this video. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Chain"
      ],
      "metadata": {
        "id": "wH2Ph0NcDlo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "RdTwSS3nHKRz"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "VGezE1qYQJ76"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "fmYnYqbWQWLi"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.invoke('is topic about plastic is discussed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGI1hEvfQvLb",
        "outputId": "56833292-1c0e-4415-c5cb-1e7a5fed3056"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"e going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of the most important interview questions will be something related to fine tuning and what is the techniques that is usually used behind right so what all things we are going to cover in this video in this video we going to talk about quantization um specifically when I say quantization it is all about model quantization because if you remember in our Lama 2 code right when we doing the fine tuning here you could see that we had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we\\n\\nthe theoretical in-depth intuition about it and that is what I'm actually going to do uh the best thing is that when I learned about this theoretical intuition and I'm doing it from past 2 to 3 months it's quite amazing guys you now this is where that machine learning era is probably coming where I used to upload a lot of theoretical in-depth geometrical intuitions regarding various machine learning algorithms similarly here here also in the series of videos in this video we are going to discuss about quantization now what exactly is quantisation we going to discuss about that in the upcoming video we are going to see techniques like Laura CLA every maths intution that is probably involved uh and these all are important for the fine tuning technique right if I probably talk about generative AI one of\\n\\nalf precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at the end of the day llms are also deep learning neural networks in the form of Transformers or Bird right then we are going to discuss about what exactly is calibration uh this is also called as like calibration in model quantization right we are going to also make sure that we are going to see some problems right how we can actually do calibration then there is different different modes of quation right uh first of all I will explain you the definition then only you'll be able to understand in modes of quation we're going to discuss about two types one is post training Quant and Quant aware training right so these all\\n\\ne had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from from a higher bit to a lower bit why we are specifically doing this I will be explaining about that right so with respect to each and every parameters definitely I will explain you the theoretical intuition and later on you just go ahead and see my previous video with respect to all the coding now you everything will make sense okay so what exactly is quation we're going to discuss you know uh we're going to discuss about full Precision half precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at\",\n",
              " 'question': 'is topic about plastic is discussed'}"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "e6osgdBfRCPN"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | chat_model | parser"
      ],
      "metadata": {
        "id": "Y3e2en89QyOC"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke('is topic is about the mobile')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ur7Ph_xlRE-7",
        "outputId": "713a20a7-418e-4037-be6d-920cf7e64d71"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'YES. \\nIn the video the speaker discusses how to use quantization to make deep learning models smaller and deploy them on mobile devices, edge devices and even watches. \\n00:54:07 - 00:56:42 \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZyERl2UwRKn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}